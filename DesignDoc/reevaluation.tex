%!TEX root = main.tex
%----------------------------------------------------------------------------------------
%   EVALUATION OF INITIAL GOALS AND REQUIREMENTS
%----------------------------------------------------------------------------------------

\section{Evaluation of Initial Goals}
Considering the project has reached completion, it is important to review the requirements initially
set forth in order to ascertain whether the objectives of the project have been met. My final year
project involved a large amount of exploratory work, and as the project developed different area's
became more important to the final deliverable than were anticipated at the beginning. For the
design document submitted in week 12 of semester one, a review of my initial requirements was
undertaken, and a re-evaluation of their status within the project was conducted. Since then further
developments of the project demanded more time and focus be spent on other aspects, as such it is
worth re-evaluating the midway evaluation, and where the project stands at completion. The key
area's of the project were.

\begin{itemize}
    \item GPU Benchmark Development
    \item Algorithm Analytics Development
    \item Optimisation Analytics Development
\end{itemize}

\subsection{GPU Benchmark Development}
As described earlier, the importance of developing working GPU benchmarking code for known problem
classes allows for better analytics and reporting in the serial algorithm analysis portions. This
therefore was a key aspect of satisfactorily completing the project. The GPU benchmark development
had a number of requirements that describe what the project necessitates.

\subsubsection{Requirements}

\paragraph{[FR.003]} \label{[FR.003]}
\textbf{The program shall run developed benchmark algorithms to further analytical information.}

This requirement relates directly to the overall aim of the project, which is described in the
requirements just proceeding this. As the project currently stands there is a completed benchmark
suite covering all of the GPU primatives in a highly extensible and meta-programmable fashion. Each
of the following benchmarks has been implemented.

\begin{itemize}
\item Peak Map
\item Peak Fold/Reduce
\item Peak Scan
\item Peak Matrix Multiplication
\end{itemize}

Additionally it is trivial to extend the benchmark suite to cover other aspects of GPU performance.

\paragraph{[OA.001]} \label{[OA.001]}
\textbf{The program shall run custom benchmark algorithms to identify GPU performance.}

This requirement was met. In order to satisfy this requirement I produced benchmarking code for GPU
performance in problem sets that are known to be performant on a GPU. It was my original intention
that as well as benchmarks that are known to be performant, that I would also write benchmarks that
may naively appear to be performant, yet further inspection demonstrates that they are not in fact
performant. This was a rather large task that did not relate back to the key intentions of the
project. It was disappointing that constraints prevented the full exploration of this concept,
however as the project developed it was clear that focusing on the positive performance aspects
would provide a significantly better final product.

\paragraph{[OA.002]}
\textbf{The program shall work on all CUDA devices.}

After careful consideration this requirement was relaxedi at the midpoint of the project. It was
determined to be far too strict. When writing the original requirements analysis I was not as
familiar with the CUDA toolchain as I was at the midpoint of the project, and as such it is now
apparent that writing Compute Capability agnostic code is a very difficult feat. In order to best
satisfy the other requirements of this project I decided to limit benchmarking code to work on CUDA
capable devices of Compute Capability 3.5 and above. This compute capability was chosen as the
capabilities of CUDA Cards differ significantly pre and post Compute Capability 3. 

The revision of the initial requirement saved significant time and effort from being wasted in
localisation and highly technical activities that would have had very limited benefits for the
project.

\paragraph{[OA.003]}
\textbf{The program shall provide comparative CPU performance metrics.}

This requirement was met. The final implementation of the benchmark suite contains both CPU and GPU
benchmarks for identical operations. This is then used to provide comparitive performance
information to the CAPA reporter in order to make predictions about performance improvements within
the codebase.

\paragraph{[OA.004]}
\textbf{The program shall provide a number of different problem class benchmark algorithms.}

This requirement was covered and expanded under sections \ref{[FR.003]} and \ref{[OA.001]}

\paragraph{[OA.005]} \label{[OA.005]}
\textbf{The program shall provide theoretical performance metrics given a known problem class}

This requirement was met. In order to provide the best possible analytics for the serial code
analysis, theoretical parallel performance must be understood, so that in situations where a GPU is
not present, that relevant calculations may be undertaken to provide an estimate on the anticipated
performance. Naturally actual performance and theoretical performance differ significantly for a
variety of reasons, however the fundamental considerations involved in algorithm analysis can be
known or reasonably estimated from which theoretical performance metrics may be provided. The
implementation of this utilised work complexity comparisons between the sequential and parallel
versions of operations.  Some aspects of the parallel code matched do not provide these
theoreticals, as in many cases the final semantics cannot be gathered from the AST representation
without runtime information.

\paragraph{[OA.006]}
\textbf{The program shall include known FPGA performance metrics given a known problem class}

This requirement was not met. During the midpoint review of the project it was clear that the
original intention of benchmarking CPU, GPU and FPGA implementations of algorithms was an
unattainable goal, this was mainly due to the increased focus on the compiler analytics aspect of
the projct. Between the original requirements analysis and the midpoint review the emphasis became
more focused upon the compiler analytics side, with less emphasis on the relative performances and
tradeoffs between the different computing architectures.  Due to the size of the project, and the
direction it began to proceed, I elected early to not satisfy the requirement, and to remove it from
what this project intends to achieve. Removing this requirement provided more time for solving
problems which were more relevant to final product.

\paragraph{Summary}
The original design of this project was to primarily provide a benchmarking and comparison suite to
assist programmers and engineers in decision making about how to best optimise their software.  Very
quickly though the project was refocused on analysing source code rather than concepts. This
decision increased the challenge of the project, but also provides more utility, as it has the
capability to be integrated far deeper in the optimisation decision process. As a result however
many aspects of the benchmarking side of the project were compromised. These compromises were
recognised before the midpoint of the project and were thoroughly documented in the Design Document.

\subsection{Algorithm Analytics Development}
This is the crux of my final year project. The core objective was to produce software that analyses
a C source file and identify whether the algorithms described may see some benefit from being
parallelised. This is extended by the other aspects of this project which in turn provide extra
metrics for comparison between CPU and GPU performance. The stretch goal was to provide analysis of
an existing C codebase which may contain a variety of potentially parallel algorithms within. As
static code analysis is a rather large task to undertake certain decisions have been made to ensure
that this project may be completed, and some of these are reflected wtihin the requirements.

\subsubsection{Requirements}

\paragraph{[FR.001]}
\textbf{The program shall analyse an algorithm and produce optimisation analysis}

This requirement was met. This was the key requirement of the entire project. This requirement could
not be compromised on, and as such all other requirements had to relate to ensuring this requirement
was met. Analysis of the algorithm was defined as static code analysis, and optimisation analysis
was defined as the recognition of potentially parallelizable algorithms within the code. This was
achieved through integrating with the Clang tooling, utilising the AST to perform the static
analysis. The static analysis itself is primrarly concerned with matching known parallel patterns
through the AST Matcher library.  

At the midpoint of the project most of this requirement had been met. However I was not pleased with
the manner in which the analysis was undertaken. It was quite fragile and difficult to parse. A
redevelopment of the entire matching interface was undertaken and as a result I created functional
combinator matcher library to facilitate the generation of generic extensible AST Matcher
constructs. This allowed for the rapid redevelopment and extension of the original analysers,
providing and extensible framework from which further matchers could easily be developed.

The fundamental intention of the requirement was to provide a list of potential improvements from
within the code, similar to runtime profiling, by using semantics as described by the designer,
rather than performance outcomes determined by a profiler. This was achieved.

\paragraph{[FR.002]} \label{[FR.002]}
\textbf{The program shall produce theoretical performance metrics of developed algorithms}

This requirement was met. This relates back to the discussion about theoretical performance metrics
in \ref{[OA.005]}. This is the extension of that requirement. Where actual benchmark information is
not available the tool provides theoretical performance based on time and work complexities of the
relevant algorithms.

\paragraph{[FR.004]}
\textbf{The source code shall be released under a FOSS license}

All source code will be released under a Modified BSD license where permitted.


\paragraph{[CA.001]}
\textbf{Integrate with the Clang tooling to analyse custom written C code}

This requirement has been completely satisfied, a stable build system has been forked from an
existing open source project providing the scaffolding around which the entirity of CAPA is built.

\paragraph{[CA.002]}
\textbf{Identify simple parallel patterns within analysed code}

This requirement has been completely satisfied, the three simple parallel patterns for which
significant improvements can be found in GPU implementations are:

\begin{itemize}
\item Map Operations
\item Reductions
\item Scans/Prefix Network
\end{itemize}

All three of these simple patterns can be successfully identified within test code.

\paragraph{[CA.003]}
\textbf{Identify medium complexity parallel patterns within analysed code}

This requirement was met. Medium complexity parallel patterns are considered to be patterns within serial code that are
clearly parallelizable yet are difficult to identify in a generic sense. This primrarly meant the
identification and tagging of 2D Matrix operations. Matrix operations are considered a medium
complexity pattern due to the variety of ways in which they may be implemented. As this aspect of
the project is primrarly pattern matching and feature detection, identifying the litany of differnet
ways a matrix multiplier may be implemented is a significant task. As such writing an accurate, yet
generic Matcher and Callback handler for this was a sizeable task. The matrix matcher however was
completed, and identifies matrix-matrix multiplication, as well as matrix-vector multiplication. 

\paragraph{[CA.004]}
\textbf{Identify non-trivial parallel patterns within analysed code}

Non-trivial parallel patterns mainly defines a broad set of problems that are not clearly
parallelizable. The original intention of this requirement was to identify graph traversals, this
however proved to be a particularly difficult task. Whilst graph traversals are not identified by
CAPA, we are still able to identify non-trivial potentially parallel sections of code. Primarily
CAPA looks for loops with minimal control structures which may be potentially unrolled or
vectorised. Additionally CAPA inspects function declarations and types to determine whether they
contain types which are to be expected in highly parallel computation, typically pointers, arrays
and unsigned integers defining size. This coupled with a low cyclometic complexity within the
definition suggests that the included code may be potentially parallelisable, and as such it is
often reported by the software. 

This requirement has been met, with a slight caveat.


\paragraph{[CA.005]} \label{[CA.006]}
\textbf{Provide Theoretical improvement information}

Has been covered extensively here \ref{[FR.002]} and here \ref{[OA.005]}.

\paragraph{[CA.006]}
\textbf{Provide more accurate theoretical improvement analysis using additional user specified information}

This requirement has been met. Expanding on \ref{[FR.002]} and \ref{[OA.005]}, if the user decides
to provide additional information, then the theoretical performance metrics will take this
information into consideration when calculating potential performance improvements. The user is
capable of providing information by way of a configuration file which CAPA reads to provide more
accurate theoretical calculations. 

\paragraph{[CA.007]}
\textbf{Analyse general C code algorithm descriptions}

This requirement has been met. The project is capable of working on any Clang compilable C codebase.
As such the analyser is capable of analysing general C code algorithm descriptions from a functional
point of view.

\paragraph{[CA.008]}
\textbf{Analyse existing codebases within tagged regions}

This requirement has been conditionally met. Tags are not always visible at every node within the
clang AST and as such I was not able to reliably analyse only subsections of a codebase, rather the
user has to select which functions within the codebase they do not want to be reported. The program
still analyses these regions of the codebase, however upon detecting an ignore clause in the parent
function comment declaration, the analysis for that match terminates and the next matcher begins.
This allows the developer to selectively remove false positives or known regions where more
computational speed is not necessary. So whilst this condition was originally to only analyse within
tagged regions, it has been changed to tag only regions which have not been tagged as:
{\lstinline{///CAPA:IGNORE}}


\subsection{Optimisation Analytics Development}

Whilst there are no specific requirements relating to this particular component, this is the
unifying feature of the entire project. Combining static code analysis with benchmarks to provide a
comprehensive optimisation report, without running a profiler allows for fast identification of
potential improvements within a codebase of any size. That is the key intention and aim of this
project, and it was achieved.

