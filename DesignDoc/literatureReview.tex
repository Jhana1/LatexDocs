%!TEX root =./main.tex -------------------------------- LITERATURE REVIEW
%--------------------------------

\section{Literature Review} 
\subsection{Introduction}
Optimisation and computational efficiency are two pillars of good program design, much research has
been undertaken in the search of improving performance and extracting hardware maximum efficiency.
Although the current literature covers an extensive range of research, this review seeks to focus
primarily on the topics of automatic vectorisation, alternative hardware (GPU/FPGA) performance in
parallel contexts, and finally the utilisation of Static Analysis and Profiling to assist in the
process of identifying potential optimisation in the massively parallel computation paradigm.
Individually these are all large topics of research, and as such this review will be focusing
primarily on computational performance, rather than power consumption or algorithm efficiency. The
purpose of this review is to provide a contextualisation around my final year project, in order to
identify potential challenges in the problem space, as elucidated by prior research. 

\subsection{Automatic Vectorisation}
Automatic vectorisation is a tool employed by many compiler designers in order to generate ASM which
utilises specialised hardware level vector instructions.  Same Instruction Multiple Data (SIMD)
instructions seek to process multiple elements of a dataset simultaneously, utilising multiple
processing units in order to achieve data level parallelism. Roger Espasa and Mateo Valero
\cite{espasa1997exploiting} explore the potential benefits of Data-Level Parallelism by
investigating computer architectures to utilise both Instruction Level and Data Level parallel
constructs. Within their research they identify and develop an architecture to utilise SIMD
instructions to leverage the performance benefits, clearly demonstrating the performance improvement
this parallel strategy provides. These techniques have since been further developed by Compiler
designers, with the LLVM/Clang team employing two forms of automatic vectorisation within their
optimising C compiler \cite{website:llvmAutoVectorize}. The Clang team focused on developing Loop
Level Vectorisation and Super Word Level Vectorisation in order to leverage parallelisation in the
target architecture.  Their automatic loop vectoriser is capable of providing an increase in
processing speed of 3 times, when tuned specifically for the Intel Core-i7 AVX instruction set. This
is a clear demonstration of the benefits already being seen by optimising compilers manipulating
sequential programs into those which leverage the power of parallel computation. The LLVM optimiser
however has some flaws which are identified by Yulei Sui, Xiaokang Fan, Hao Zhou, and Jingling Xue
who developed Loop-oriented array and field sensitive Pointer Analysis (LPA) in order to combat the
suboptimal performance of the LLVM auto-vectoriser \cite{sui2016loop}. The authors identified alias
analysis as being a potential cause of the LLVM compiler missing optimisation opportunities. They
created an analysis framework around both flow-insensitive and flow-sensitive pointer construct. By
hooking into the LLVM’s partial SSA form they exploited the reduced semantic complexity to
algorithmically generate superior memory patterns, and by extension developed a superior loop
vectoriser, with performance up to 10\% better than the original LLVM. This research again
demonstrates the performance opportunities created by parallel computation, however they are all
fundamentally limited by the CPU’s capacity to perform SIMD operations. Most CPU architectures have
at most 8 complex computing cores, limiting the maximum potential throughput, however GPU
architectures have the capacity for massively parallel computation. The typical design inherently
leverage SIMD concepts containing thousands of simple computational cores with high memory
bandwidth. Thus whilst automatic vectorisation has substantial performance potential, most current
literature is focused on continuing to improve computational efficiency of host bound programs,
rather than looking towards exploiting the massively parallel computational power of modern GPU’s.

\subsection{Parallelism}
General Purpose GPU programming seeks to exploit the parallel performance characteristics of the GPU
architecture; identification and development of algorithms which leverage this design pattern can
provide substantial computational speedups. The authors of \cite{papadrakakis2011new} explore the
fundamentals of GPGPU programming, and the CUDA architecture. Inherent within the CUDA architecture
is hybrid CPU-GPU programming to produce the most efficient solution. In the example described the
authors extract maximum parallelism from Matrix operations, and leave complex control flow to the
CPU, thus developing a solution which maximises the performance of the individual components of the
Hybrid Host-Device model. The proposed solution utilises parallel primitives such as the Reduction,
which exploits parallelism by utilising a summation tree. This requires that the data and binary
operator form a semigroup (The set of data must be closed under an associative binary operator). In
their example the set is of integers, and the operator is addition, which holds these properties.
The concept of parallel primitive operations is further expanded by \cite{nagrajan2011accelerating}
which provides terminology to describe parallel patterns for both computation and communication.
Although this research focuses on parallelism within FPGA’s the primitive operations are examples of
fundamentally parallel operations, exposing high levels of optimisation potential through SIMD
data-paths. The authors look to extract performance from their FPGA implementation of potentially
parallelisable algorithms; their key focus is on the Parzen window technique of Gaussian PDF
estimation, as well as K-Means clustering. From their research they develop a concept of
pattern-based algorithm decomposition; as a means of exposing potential parallelism within a
codebase to individuals with little or no experience with highly parallel code.  The authors
describe a limitation in the existing development framework, where FPGA based systems are developed
on by only highly specialised and skilled developers. This concept is explored by the authors of
\cite{sengupta2007scan} in which they describe the limitation upon many programmers is
the lack of a breadth of libraries which exploit the benefits of GPGPU programming.  Within this
paper they present the problem of fragmentation within the GPU programming space, with competing
standards and API’s resulting in specialists rolling their own solutions to many problems, resulting
in minimal code re-use. The current solution to the code reuse problem is the introduction of
parallel algorithms within the C++ STL as well as CUDA based libraries such as CuBLAS and Thrust,
which aim to provide programmers with a foundation from which to build larger programs, without
being forced into having a full understanding of programming on a GPU. Given the power and
performance characteristics of GPU’s there is a great incentive to move computationally expensive
algorithms onto these devices, currently however there are many roadblocks preventing individuals
and organisations from exploiting the potential improvements within their codebase, the high barrier
to entry can be difficult to overcome, especially with the limited capabilities of identifying how
beneficial any redevelopment may actually be.

\subsection{Static Analysis}
Static Analysis is the analysis of the original source statements of a program, it provides a method
by which the semantics of a program may be identified.  Typically, Static Analysis is utilised in
order to identify bugs within a codebase \cite{bessey2010few}, there is a litany of literature which
describes a variety of processes through which one can employ Static Analysis to search out and find
bugs \cite{ball2001automatically} \cite{bush2000static}. These tools rely on parsing the source of a
program and identifying the anti-patterns within, alerting developers to their potential mistakes.
Additionally, many static analysers provide complexity statistics about the analysed source; it is
often the intention of static analysers to provide the developer with a summary of information about
their codebase which facilitates further investigation and development.  Although there has been a
large amount of research into static analysis for the purpose of detecting and eliminating bugs,
there is a lack of research into the topic of utilising static analysis for the purpose of
performance improvements.  Static analysis as a tool is rarely used to facilitate optimisation
improvements within a codebase; programmers often utilise profiling in order to determine where to
invest development time focused on optimisation.  Clearly this presents a divide, where programmers
often utilise Static Analysis and Profiling in a competing demands structure for developer time.
Within the NVidia High-Productivity CUDA Programming presentation \cite{nvidiapresentation}
they recommend the utilisation of a process called “APOD” Assess-Parallelise-Optimise-Deploy.
Within the Assessment and Optimisation phase of the process they recommend utilising profilers in
order to make determinations about what aspect of the code requires further development. The
weakness of profiling however is that it is often very time consuming, additionally it requires an
individual developer have an understanding of how to potentially translate sequential serial
algorithms into their massively parallel equivalents. 
 

